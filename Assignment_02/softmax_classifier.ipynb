{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All necessary imports at the beginning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata       \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, explore and prepare dataset\n",
    "\n",
    "The MNIST dataset is a classic Machine Learning dataset you can get it and more information about it from the website of [Yann Lecun](http://yann.lecun.com/exdb/mnist/). MNIST contains handwrittin digits and is split into a tranings set of 60000 examples and a test set of 10000 examples. You can use the module ```sklearn``` to load the MNIST dataset in a convenient way. \n",
    "easy load, mldata.org, orginal mnist, mnist link and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits (70000, 784)\n",
      "labels (70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original') #load MNIST\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] #separate images and labels\n",
    "\n",
    "# shape of MNIST data\n",
    "print('digits', X.shape)\n",
    "print('labels',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visualization of MNIST we will plot a digit. Each line represents an image in flatten form (all pixel in a row). We have change the shape from a vector back to a matrix of the original shape to plot the image. In the case of MNIST this means a conversion of 784 pixel into 28x28 pixel. In addition we will check the label of that digit to verify it correspond to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 8.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUJ4z9gpzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11343f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_mnist_digit(digit):\n",
    "    image = digit.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='binary', interpolation='bicubic')\n",
    "\n",
    "#choose a random number, plot it and check label \n",
    "random_number = np.random.randint(1,60001)\n",
    "print('label:',y[random_number]) \n",
    "plot_mnist_digit(X[random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a glimpse into MNIST let us explore it a bit further. Write a function ``` plot_mnist_digits(data, examples_each_row)``` that plots configurable number of examples for each class, like:\n",
    "![MNIST Examples](images/MNIST_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mnist_digits(data, examples_each_row):\n",
    "    ############################################\n",
    "    #TODO: Write a function that plots as many #    \n",
    "    #      examples of each class as defiend   #\n",
    "    #      by 'examples_each_row'              #\n",
    "    ############################################\n",
    "    if (examples_each_row > len(data[0])):\n",
    "        print(\"examples_each_row is greater than row length, instead printing all elements\")\n",
    "        for row in range(data):\n",
    "            for example in data[row]:\n",
    "                plot_mnist_digit(data[row][example])\n",
    "    else if (examples_each_row > 0):\n",
    "        for row in range(data):\n",
    "            for example in range(0, examples_each_row - 1):\n",
    "                plot_mnist_digit(data[row][example])\n",
    "    else:\n",
    "        print(\"examples_each_row < 0, no examples are printed\")\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "plot_mnist_digits(X, examples_each_row=11)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring MNIST let us prepare the date for our linear classifier. First we need to separate traning and test data. Further we will shuffle the traning data to get a random distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into training and test set\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a linear classifier using Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model to classify the MNIST dataset with the following equation:\n",
    "\n",
    "$$ L = \\frac{1}{M} \\sum_{i=1}^{M} -log\\; \\left ( \\frac{e^{h(x_j,\\Theta)}}{\\sum_{k=1}^{K}e^{h(x_k,\\Theta)}} \\right)_i + \\frac{\\lambda}{2} \\sum_{}^{} \\Theta^2, \\: with \\;\\; h(X,\\Theta) = X * \\Theta $$\n",
    "\n",
    "Using the universal equation for a loss function we can see the separate parts of that hugh equation.  \n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_i L_i(h(x_i,\\Theta),y_i) + \\lambda R(\\Theta)$$\n",
    "\n",
    "We will implement each part on its own and put them together. That way it is much easier to understand whats going on. Let us start with the score function or hypothesis:\n",
    "\n",
    "$$h(X,\\Theta) = X * \\Theta$$\n",
    "\n",
    "It is possible to calculate all score values with one matrix multiplication ([dot product](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.dot.html)) so we can use the whole traning data $X$ instead of one digit $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_scores(X,theta):\n",
    "    ############################################\n",
    "    #TODO: Implement the hypothesis and return #\n",
    "    #      the score values for each class of  #\n",
    "    #      every digit.                        #\n",
    "    ############################################\n",
    "    return np.dot(X, theta)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we can define the data loss funtion $L_i$. We assume the score values are unnormalized log probabilities and we use the softmax function to calculate probabilities.\n",
    "$$ P(Y=j\\mid X=x_i) = \\frac{e^{s_j}}{\\sum_{k=1}^{K}e^{s_k}} $$\n",
    "$$ L_i = -log\\;P(Y=j\\mid X=x_i) $$\n",
    "\n",
    "Hint: If the correct classes (labels) are in a [one hot encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) shape you can use a matrix multiplication to extract the correct class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support function to convert label vector into a one hot encoding matrix\n",
    "def onehot_encode_label(label):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    label = label.reshape(len(label), 1)\n",
    "    onehot_encoded_label = onehot_encoder.fit_transform(label)\n",
    "    return onehot_encoded_label\n",
    "\n",
    "# Calculate class probability distribution for each digit from given class scores\n",
    "def softmax(class_scores):\n",
    "    ############################################\n",
    "    #TODO: Use the softmax function to compute #\n",
    "    #      class probabilties                  #\n",
    "    ############################################\n",
    "    return np.exp(class_scores) / np.sum(np.exp(class_scores), axis=0)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "\n",
    "# Compute data_loss L_i for the correct class\n",
    "def data_loss(class_probabilities, onehot_encode_label):\n",
    "    ############################################\n",
    "    #TODO: With hot encoded labels and class   #\n",
    "    #      probabilties calculate data loss    #\n",
    "    #      L_i                                 #\n",
    "    ############################################\n",
    "    return - np.log(class_probabilities)         # Wofür wird hier onehot_encode_label benötigt?\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate loss $L$ using the defined functions. \n",
    "\n",
    "$$ L = \\frac{1}{M} \\sum_i L_i(h(x_i,\\Theta),y_i) + \\lambda R(\\Theta)$$\n",
    "\n",
    "Besides the loss L we will have to calculate the gradient for our loss function $L$. To minimize our loss we will need the gradient. For more information about the gradient you can use additional sources, like that good [blog post](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 1\n",
    "encoded_labels = onehot_encode_label(y)           # also needed for the gradient, therefore separated calculated\n",
    "probabilities = softmax(class_scores(X,theta))    # also needed for the gradient, therefore separated calculated\n",
    "loss_Li = data_loss(probabilities,encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(X, y, theta, lam):\n",
    "    encoded_labels = onehot_encode_label(y)           # also needed for the gradient, therefore separated calculated\n",
    "    probabilities = softmax(class_scores(X,theta))    # also needed for the gradient, therefore separated calculated\n",
    "    loss_Li = data_loss(probabilities,encoded_labels) \n",
    "    \n",
    "    m = X.shape[0]                                    # number of training data for normalization\n",
    "    l2_regularization = (lam/2)*np.sum(theta*theta)   # regularization loss\n",
    "  \n",
    "    ############################################\n",
    "    #TODO: Put everthing together and calculte #\n",
    "    #      loss L and gradient dL with given   #\n",
    "    #      variables.                          #\n",
    "    ############################################\n",
    "    loss = 1/m * np.sum(loss_Li) + l2_regularization\n",
    "    gradient = probabilities * (encoded_labels - probabilities)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (70000,784) and (70000,10) not aligned: 784 (dim 1) != 70000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-696f6e376276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-83ecf74ab4b1>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y, theta, lam)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mencoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot_encode_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# also needed for the gradient, therefore separated calculated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# also needed for the gradient, therefore separated calculated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss_Li\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                                    \u001b[0;31m# number of training data for normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-690806018ddd>\u001b[0m in \u001b[0;36mdata_loss\u001b[0;34m(class_probabilities, onehot_encode_label)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#      L_i                                 #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monehot_encode_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#             END OF YOUR CODE             #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (70000,784) and (70000,10) not aligned: 784 (dim 1) != 70000 (dim 0)"
     ]
    }
   ],
   "source": [
    "loss(X,y,1,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the cost using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(traning_data, traning_label, theta, lam=0.5, iterations=100, learning_rate=1e-5):\n",
    "    losses = []\n",
    "    ############################################\n",
    "    #TODO: Optimize loss with gradient descent #\n",
    "    #      update rule. Return a final model   #\n",
    "    #      and a history of loss values.       #\n",
    "    ############################################\n",
    "    \n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################    \n",
    "    return theta, losses\n",
    "\n",
    "# Initialize learnable parameters theta \n",
    "theta = np.zeros([X_train.shape[1],len(np.unique(y_train))])\n",
    "# Start optimization with traning data, theta and optional hyperparameters\n",
    "opt_model, loss_history = gradient_descent(X_train,y_train,theta,iterations=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "Let us look at the optimization results. Final loss tells us how far we could reduce costs during traning process. Further we can use the first loss value as a sanity check and validate our implementation of the loss function works as intended. Recall loss value after first iteration should be $ log\\:c$ with $c$ being number of classes. To visulize the whole tranings process we can plot losss values from each iteration as a loss curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check loss after last iteration\n",
    "print('last iteration loss:',loss_history[-1])\n",
    "# Sanity check: first loss should be ln(10)\n",
    "print('first iteration loss:',loss_history[0])\n",
    "# Plot a loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation above gave us some inside about the optimization process but did not quantified our final model. One possibility is to calculate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelAccuracy(X,y,theta):\n",
    "    # calculate probabilities for each digit\n",
    "    probabilities = softmax(np.dot(X,theta))\n",
    "    # class with highest probability will be predicted\n",
    "    prediction = np.argmax(probabilities,axis=1)\n",
    "    # Sum all correct predictions and divied by number of data\n",
    "    accuracy = (sum(prediction == y))/X.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "print('Training accuracy: ', modelAccuracy(X_train,y_train,opt_model))\n",
    "print('Test accuracy: ', modelAccuracy(X_test,y_test,opt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that quantification is limited. A more gerenell approach is to calculate a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and get different model measurements from it. A good overview for model measurements is provided by the wikipedia article of [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). We implement a confusion matrix for our model and calculate a [F1 score](https://en.wikipedia.org/wiki/F1_score) and ```print()``` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusionMatrix(X,y,theta):\n",
    "    ############################################\n",
    "    #TODO: Calculate a confusion matrix for    # \n",
    "    #      and it.                             #\n",
    "    ############################################\n",
    "    return None\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "def f1Score(confMatrix):\n",
    "    ############################################\n",
    "    #TODO: Calculate a F1 score from a given   #\n",
    "    #      confusion matrix.                   #\n",
    "    ############################################\n",
    "    return None\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting is to plot a part of $theta$, because you can visualize the learned templates for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(opt_model[:,0],[28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
