{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Fundamentals - Neural Networks - Exercise: Minimal Fully Connected Network for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Requirements](#Requirements) \n",
    "  * [Modules](#Python-Modules) \n",
    "  * [Data](#Data)\n",
    "* [Simple MNIST Network](#Simple-MNIST-Network)\n",
    "  * [Todo: Transparency](#Todo:-Transparency)\n",
    "  * [Todo: Comprehension](#Todo:-Comprehension)\n",
    "  * [Todo: Step towards a NN-Framework](#Todo:-Step-towards-a-NN-Framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# third party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# internal\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data')\n",
    "\n",
    "# load all data, labels are one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(one_hot_enc=True, normalized=True)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MNIST Network\n",
    "The presented network is an adaptation of Michael Nielson's introductory example to neural networks. It is recommended, though not necessary, to read the first two chapters of his great online book ['Neural Networks and Deep Learning'](http://neuralnetworksanddeeplearning.com/) for a better understanding of the given example. Compared to the [original](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py) by Nielsen, the present variant was vectorized and the sigmoid activation function replaced by a rectified linear unit function (ReLU). As a result, the code is written much more compact, and the optimization of the model is much more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo: Transparency\n",
    "Your goal is to understand how the implementation works. Therefore your tasks are as follows:\n",
    "  - (2) Add comments to functions and lines of code. Follow the [Google-Pyhton](https://google.github.io/styleguide/pyguide.html) guidelines for comments.\n",
    "  - (2) Add a verbose argument (`boolean`) to the functions that adds meaningful `print` lines to the network, if it is `true`.\n",
    "  - (2) Add a variable `delta_hist` which store the delta value calculated on the output layer during each iteration of the function `grads(X,Y,weights)`. After the optimization process plot `delta_hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feed_forward(X, weights):\n",
    "    \"\"\"Calculate the outputs of all layer of the network.\n",
    "\n",
    "    Calculates the outputs of all layers of the network given \n",
    "    the inputs X and the weights. As activation function ReLU \n",
    "    is used.\n",
    "\n",
    "    Args:\n",
    "        X: The input values of the network.\n",
    "        weights: The weights of all layers of the network.\n",
    "\n",
    "    Returns:\n",
    "       A numpy.array containing the outputs of all layers of \n",
    "       the network.\n",
    "    \"\"\"\n",
    "    a = [X]\n",
    "    for w in weights:\n",
    "        a.append(np.maximum(a[-1].dot(w),0))\n",
    "    return a\n",
    "\n",
    "def grads(X, Y, weights, delta_hist):\n",
    "    \"\"\"Calculate the gradients of the weights of the network.\n",
    "\n",
    "    Calculate the gradients of the weights of the network given \n",
    "    the inputs X, the labels of the inputs Y and the weights. \n",
    "    Therefor the quadratic cost function is used.\n",
    "    \n",
    "    Args:\n",
    "        X: The input values of the network.\n",
    "        Y: The labels of the input values of the network.\n",
    "        weights: The weights of all layers of the network.\n",
    "\n",
    "    Returns:\n",
    "       A numpy.array containing all gradients of the weights \n",
    "       of the network.\n",
    "    \"\"\"\n",
    "    # Init grads\n",
    "    grads = np.empty_like(weights)\n",
    "    # Calculates the output of the layer\n",
    "    a = feed_forward(X, weights)\n",
    "    # https://brilliant.org/wiki/backpropagation/ or https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
    "    # Calculates delta for the output layer\n",
    "    delta = a[-1] - Y # why not * (a[i] > 0) bzw. * (z[i] > 0)?\n",
    "    delta_hist.append(np.sum(delta*Y)/len(X))  # why np.sum(delta*Y)/len(X) and not just delta or np.sum((Y - a[-1])*(Y - a[-1]))/(len(X)*2)?\n",
    "    # Calculates gradients for the output layer\n",
    "    grads[-1] = a[-2].T.dot(delta)\n",
    "    for i in range(len(a)-2, 0, -1):\n",
    "        # Calculates delta for all other layers\n",
    "        delta = (a[i] > 0) * delta.dot(weights[i].T) # why not * (z[i] > 0)?\n",
    "        # Calculates gradients for all other layers\n",
    "        grads[i-1] = a[i-1].T.dot(delta)\n",
    "    return grads / len(X), delta_hist # why / len(x)?\n",
    "\n",
    "def optimize(trX, trY, teX, teY, weights, num_epochs=20, batch_size=50, learn_rate=0.1, verbose=True):\n",
    "    ''' Implements a weight optimasation with SGD.\n",
    "    \n",
    "    Args:\n",
    "        trX: Trainig data\n",
    "        trY: Trainig labels\n",
    "        teX: Test data\n",
    "        teY: Test labels \n",
    "        weights: weights\n",
    "        num_epochs: numbers of epochs\n",
    "        batch_size: batch size\n",
    "        learn_rate: learning rate\n",
    "        verbose: If True information about the process are printed during the optimisation.\n",
    "        \n",
    "    Returns:\n",
    "       Optimized weights\n",
    "    '''\n",
    "    delta_hist = []\n",
    "    # Iterate over epoches\n",
    "    for i in range(num_epochs):\n",
    "        # Iterate over batches\n",
    "        for j in range(0, len(trX), batch_size):\n",
    "            # Generate batches\n",
    "            X, Y = trX[j:j+batch_size], trY[j:j+batch_size]\n",
    "            # Update wights\n",
    "            _grads, delta_hist = grads(X, Y, weights, delta_hist)\n",
    "            weights -= learn_rate * _grads\n",
    "        \n",
    "        if verbose:\n",
    "            # Test model  \n",
    "            prediction_test = np.argmax(feed_forward(teX, weights)[-1], axis=1)\n",
    "            if verbose:\n",
    "                print (i, np.mean(prediction_test == np.argmax(teY, axis=1)))\n",
    "        \n",
    "    return weights, delta_hist\n",
    "\n",
    "# Data inistalization \n",
    "trX, trY, teX, teY = train_images, train_labels, test_images, test_labels\n",
    "# Weights inistalization with gaussian normal distribution\n",
    "weights = [np.random.randn(*w) * 0.1 for w in [(784, 200), (200,100), (100, 10)]]\n",
    "# Optimize weights\n",
    "weights, delta_hist = optimize(trX, trY, teX, teY, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(delta_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo: Comprehension\n",
    "Hopefully, this implementation of a neural network is clear now. As a check answer the following questions (a few sentences, no novels):\n",
    "  - (2) Which cost function is used, what is its derivation and how is it implemented?\n",
    "      * Quadratic cost functionis used and the derivation is the difference between the true value (y) and the predicted value (a) multiplied by the input of the layer. If the input is lesser or equal zero, zero is takes as the input value. As formular : $$ \\begin{eqnarray} \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\end{eqnarray} $$ $$ \\begin{eqnarray} \\delta^L = (a^L-y)\\end {eqnarray} $$ But this is only true for the output layer. The other layers need to take into account the delta of their following layers by multiplying this value with the weights. As formula: $$ \\begin{eqnarray} \\delta^l = ((w^{l+1})^T \\delta^{l+1})  \\end{eqnarray}\n",
    "$$\n",
    "      * It is implemeted as ```delta = a[-1] - Y; a[-2].T.dot(delta)``` for the output layer and for all other layers as ```delta = (a[i] > 0) * delta.dot(weights[i].T); a[i-1].T.dot(delta)```. Where `a` contains the output values of all layers of the network; except the values of the index ```0```: these are the input values of the network. The index ```-1``` means that the outputs of the last layer of the network are used. ```Y``` are the exact values ```a[-1]``` should have if the model would work perfectly.\n",
    "  - (2) Why are the boundaries of your plot between [-1,0], why it is so noisy, how do you can reduce the noice and what is the difference to a usual plot of a loss function?\n",
    "      * The bounderies are between [-1,0] because not the acctual delta is plottet but the gradient.\n",
    "      * It is so noisy because of SGD. Another optimization algorithm could be used.\n",
    "      * The difference to usual plots is that it has a negative value range and thet the value is getting bigger over time where usual loss values are getting smaller over time.\n",
    "  - (2) How does the network implement the backpropagation algorithm? \n",
    "      * It first computes the gradients of the output (last) layer and goes than backwards through the network and compute the gradients for each following layer. The computation of the gradient of the output layer is a bit different to the computaion of all other layers. That is due to the fact that the output layer have not following layer but the others have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo: Step towards a NN-Framework\n",
    "The presented implementation is compact and efficient, but hard to modify or extend. However, a modular design is crucial if you want to experiment with a neural network to understand the influence of its components. Now you make the first changes towards your own 'toy-neural-network-framework', which you should expand in the progress of exercise 03. \n",
    "\n",
    "(5) Rework the implementation from above given the classes and methods below. Again, you _do not_ have to re-engineer the whole neural network at this step. Rework the code to match the given specification and do necessary modifications only. For your understanding, you can change the names of the variables to more fitting ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(*layer) * 0.1 for layer in self.layers]\n",
    "        self.a = None\n",
    "        self.grads = None\n",
    "        self.delta_hist = []\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"Calculates the outputs of all layer of the network.\n",
    "\n",
    "        Calculates the outputs of all layers of the network for \n",
    "        the given data. As activation function ReLU is used.\n",
    "\n",
    "        Args:\n",
    "            data: The input values of the network.\n",
    "\n",
    "        Returns:\n",
    "           A numpy.array containing the outputs of all layers of \n",
    "           the network.\n",
    "        \"\"\"    \n",
    "        a = [data]\n",
    "        for w in self.weights:\n",
    "            a.append(self.relu(a[-1].dot(w)))\n",
    "        return a\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"Calculates the gradients of the weights of the network.\n",
    "\n",
    "        Calculates the gradients of the weights of the network given \n",
    "        the inputs X and the labels of the inputs Y. Therefor the \n",
    "        quadratic cost function is used.\n",
    "    \n",
    "        Args:\n",
    "            X: The input values of the network.\n",
    "            Y: The labels of the input values of the network.\n",
    "\n",
    "        Returns:\n",
    "           A numpy.array containing all gradients of the weights \n",
    "           of the network.\n",
    "        \"\"\"\n",
    "        self.grads = np.empty_like(self.weights)\n",
    "        self.a = self.forward(X)\n",
    "        delta = self.a[-1] - Y \n",
    "        self.delta_hist.append(np.sum(delta*Y)/len(X))\n",
    "        self.grads[-1] = self.a[-2].T.dot(delta)\n",
    "        \n",
    "        for i in range(len(self.a) - 2, 0, -1):\n",
    "            delta = self.relu_prime(self.a[i]) * delta.dot(self.weights[i].T)\n",
    "            self.grads[i-1] = self.a[i-1].T.dot(delta)\n",
    "        \n",
    "        return self.grads / len(X)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predicts the labels for given data.\n",
    "\n",
    "        Predicts the labels for the given data.\n",
    "    \n",
    "        Args:\n",
    "            data: The data which labels are predicted for.\n",
    "\n",
    "        Returns:\n",
    "           A numpy.array containing all labels as indices.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.forward(data)[-1], axis=1)\n",
    "    \n",
    "    def relu(self, data):\n",
    "        \"\"\"Calculates the activation values for the ReLU function.\n",
    "\n",
    "        Calculates the activation values for the ReLU function for \n",
    "        the given data.\n",
    "    \n",
    "        Args:\n",
    "            data: The data which the activation values are \n",
    "            calculated for.\n",
    "\n",
    "        Returns:\n",
    "           A numpy.array containing all activation values.\n",
    "        \"\"\"\n",
    "        return np.maximum(data, 0)\n",
    "    \n",
    "    def relu_prime(self, data):\n",
    "        \"\"\"Calculates the gradients for the ReLU function.\n",
    "\n",
    "        Calculates the gradients for the ReLU function given \n",
    "        the wighted inputs. \n",
    "    \n",
    "        Args:\n",
    "            data: The data which the gradients are \n",
    "            calculated for.\n",
    "\n",
    "        Returns:\n",
    "           A numpy.array containing all gradients.\n",
    "        \"\"\"\n",
    "        return (data > 0)\n",
    "            \n",
    "class Optimizer:\n",
    "    def __init__(self, network, train_data, train_labels, test_data=None, test_labels=None, epochs=100, batch_size=20, learning_rate=0.01, verbose=False):\n",
    "        self.network = network\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sgd(verbose)\n",
    "        \n",
    "    def sgd(self, verbose=False):\n",
    "        \"\"\"Optimizes a network using SGD.\n",
    "\n",
    "        Optimizes a network using SGD. \n",
    "    \n",
    "        Args:\n",
    "            verbose: If True information about the process are \n",
    "            printed during the optimisation.\n",
    "        \"\"\"\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(0, len(self.train_data), self.batch_size):\n",
    "                X, Y = self.train_data[j:j+self.batch_size], self.train_labels[j:j+self.batch_size]\n",
    "                self.network.weights -= self.learning_rate * self.network.backward(X, Y)\n",
    "            if verbose:\n",
    "                print(i, np.mean(self.network.predict(self.test_data) == np.argmax(self.test_labels, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following code should run:    \n",
    "mnist_NN = FullyConnectedNetwork([(784, 200),(200,100),(100, 10)]) \n",
    "epochs, batch_size, learning_rate = 20, 500, 0.1\n",
    "Optimizer(mnist_NN, train_images, train_labels, test_images, test_labels, epochs, batch_size, learning_rate, True)\n",
    "plt.plot(mnist_NN.delta_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
