{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Fundamentals - Neural Networks - Exercise: Neural Network Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Your main goal is to extend the existing framework, to perform experiments with different model combinations and to document your observations. Here is a list of necessary tasks and some ideas for additional points:\n",
    "  * (6) Write a data loader for a different image dataset, e.g., CIFAR or Labelled Faces in the Wild. Feel free to search a dataset you like to classify. Create and train a simple fully connected network on that dataset in this notebook.\n",
    "  * (10) Implement the `Conv` and `Pool` Layer in `layer.py`. Create and train a convolutional neural network on Mnist and your chosen dataset in this notebook.\n",
    "\n",
    "Bonus points\n",
    "  * (5) 1 to 5 points are given for improving the class and method comments in the framework files. Points are given based on the quality and quantity of the comments.\n",
    "  * (1) For each additional implemented activation functions in `activation_func.py` you get 1 bonus point (max 4 points). Test your implementation in this notebook and observe effects on your networks. Keep an eye on your layer initialization.\n",
    "  * (2) Implement `Dropout` in `layer.py` and test your implementation with a toy example. Create and train a model that includes Dropout as a layer.\n",
    "  * (5) Implement `Batchnorm` in `layer.py` and test your implementation with a toy example. Create and train a model that includes Dropout as a layer.\n",
    "  * (4) Implement another optimization algorithm in `optimizer.py`. Train one of your models with that new optimizer.\n",
    "  * (5) Do something extra, up to 5 points.  \n",
    "  \n",
    "Please document thoroughly and explain what you do in your experiments, so that work in the notebook is comprehensible, else no points are given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fragen\n",
    "* Sollen wir nur für Pool Layer nur Max forward und backward implementieren? Falls nein, wie solen wir das mit der backward implementierung machen?\n",
    "* Siehe in layers.py: \n",
    "    * In Pooling.backward: What is if not only one pixel has the max value?\n",
    "    * In Conv Layer: soll zu jedem Pixel bias hinzugefügt werden oder erst nach sum?\n",
    "    * In Dropout: maske für alle images gleich oder für jedes eine andere? Und wie sollen wir damit umgehen, wenn wir vorhersagen wollen?\n",
    "    * Braucht man nach Dropout oder Pool Layer eine Aktivierungsfunktion?\n",
    "    * Pooling Layer gibt immer gleichen loss zurück, woran kann das liegen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# custom \n",
    "from htw_nn_framework.networks import NeuralNetwork\n",
    "from htw_nn_framework.layer import *\n",
    "from htw_nn_framework.activation_func import *\n",
    "from htw_nn_framework.loss_func import *\n",
    "from htw_nn_framework.optimizer import *\n",
    "from htw_nn_framework.cifar import *\n",
    "from htw_nn_framework.initializer import *\n",
    "\n",
    "# third party\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "(60000, 28, 28) (60000,)\n",
      "(60000, 1, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data')\n",
    "\n",
    "# load all data, labels are one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(flatten=False, one_hot_enc=False, normalized=True)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# reshape to match generell framework architecture \n",
    "train_images, test_images = train_images.reshape(60000, 1, 28, 28), test_images.reshape(10000, 1, 28, 28)            \n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Fully Connected Network Example\n",
    "This model and optimization is taken from `framework_exercise.ipynb` as an example for a typical pipeline using the framework files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_batch, train_labels_batch, test_images_batch, test_labels_batch = train_images[:100,:,:,:], train_labels[:100], test_images[:100,:,:,:], test_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.374715656182273 :: Training = 0.36 :: Test = 0.22\n",
      "Epoch 2\n",
      "Loss = 2.395817319196847 :: Training = 0.37 :: Test = 0.28\n",
      "Epoch 3\n",
      "Loss = 1.715375971645197 :: Training = 0.6 :: Test = 0.34\n",
      "Epoch 4\n",
      "Loss = 1.6932746995957566 :: Training = 0.59 :: Test = 0.4\n",
      "Epoch 5\n",
      "Loss = 1.6564738644118353 :: Training = 0.83 :: Test = 0.57\n",
      "Epoch 6\n",
      "Loss = 0.9413432878258848 :: Training = 0.88 :: Test = 0.66\n",
      "Epoch 7\n",
      "Loss = 0.29728511913975375 :: Training = 0.96 :: Test = 0.71\n",
      "Epoch 8\n",
      "Loss = 0.2420351258434986 :: Training = 1.0 :: Test = 0.72\n",
      "Epoch 9\n",
      "Loss = 0.057624874393098525 :: Training = 1.0 :: Test = 0.76\n",
      "Epoch 10\n",
      "Loss = 0.06200791410649452 :: Training = 1.0 :: Test = 0.76\n"
     ]
    }
   ],
   "source": [
    "# Design a three hidden layer architecture with dense layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images_batch, train_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=test_images_batch, y_test=test_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: Your Extensions and Experiments\n",
    "# MNIST\n",
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.2850633905218087 :: Training = 0.15 :: Test = 0.16\n",
      "Epoch 2\n",
      "Loss = 2.191314623755756 :: Training = 0.36 :: Test = 0.23\n",
      "Epoch 3\n",
      "Loss = 2.090857471811215 :: Training = 0.45 :: Test = 0.22\n",
      "Epoch 4\n",
      "Loss = 1.9592645047913169 :: Training = 0.53 :: Test = 0.3\n",
      "Epoch 5\n",
      "Loss = 1.7882813482648166 :: Training = 0.61 :: Test = 0.32\n",
      "Epoch 6\n",
      "Loss = 1.564081062047945 :: Training = 0.72 :: Test = 0.38\n",
      "Epoch 7\n",
      "Loss = 1.2974976819475923 :: Training = 0.81 :: Test = 0.53\n",
      "Epoch 8\n",
      "Loss = 1.055454023142705 :: Training = 0.9 :: Test = 0.58\n",
      "Epoch 9\n",
      "Loss = 0.8820211329782413 :: Training = 0.93 :: Test = 0.64\n",
      "Epoch 10\n",
      "Loss = 0.7107594834843316 :: Training = 0.95 :: Test = 0.68\n"
     ]
    }
   ],
   "source": [
    "# Design a one hidden layer architecture with conv layer\n",
    "# and ReLU as activation function\n",
    "def cnn_mnist():\n",
    "    hidden_01 = Conv(train_images_batch.shape, 1, 3, 1, True)\n",
    "    relu_01 = ReLU()\n",
    "    flat = Flatten()\n",
    "    hidden_02 = FullyConnected(784, 1500)\n",
    "    relu_02 = ReLU()\n",
    "    ouput = FullyConnected(1500, 10)\n",
    "    return [hidden_01, relu_01, flat, hidden_02, relu_02, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "cnn = NeuralNetwork(cnn_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "cnn = Optimizer.sgd(cnn, train_images_batch, train_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.001, X_test=test_images_batch, y_test=test_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.489811424969117 :: Training = 0.51 :: Test = 0.43\n",
      "Epoch 2\n",
      "Loss = 1.486813771462919 :: Training = 0.79 :: Test = 0.66\n",
      "Epoch 3\n",
      "Loss = 0.8937381651867587 :: Training = 0.6 :: Test = 0.44\n",
      "Epoch 4\n",
      "Loss = 1.539132698737083 :: Training = 0.9 :: Test = 0.71\n",
      "Epoch 5\n",
      "Loss = 0.2405226618158861 :: Training = 0.97 :: Test = 0.7\n",
      "Epoch 6\n",
      "Loss = 0.6782434847612457 :: Training = 0.78 :: Test = 0.63\n",
      "Epoch 7\n",
      "Loss = 2.999916865416738 :: Training = 0.74 :: Test = 0.53\n",
      "Epoch 8\n",
      "Loss = 0.9693661839482216 :: Training = 0.94 :: Test = 0.75\n",
      "Epoch 9\n",
      "Loss = 0.15672145680932356 :: Training = 0.98 :: Test = 0.74\n",
      "Epoch 10\n",
      "Loss = 0.07406938797170083 :: Training = 1.0 :: Test = 0.78\n"
     ]
    }
   ],
   "source": [
    "# Design a one hidden layer architecture with conv layer, pooling layer\n",
    "# and ReLU as activation function\n",
    "def cnn_pool_mnist():\n",
    "    hidden_01 = Conv(train_images_batch.shape, 1, 3, 1, True)\n",
    "    relu_01 = ReLU()\n",
    "    pool = Pool(train_images_batch.shape, np.max, 2, 1)\n",
    "    flat = Flatten()\n",
    "    ouput = FullyConnected(729, 10)\n",
    "    return [hidden_01, relu_01, pool, flat, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "cnn_pool = NeuralNetwork(cnn_pool_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "cnn_pool = Optimizer.sgd(cnn_pool, train_images_batch, train_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=test_images_batch, y_test=test_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Conected Neural Network with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.8152648410173406 :: Training = 0.58 :: Test = 0.4\n",
      "Epoch 2\n",
      "Loss = 1.653268114374418 :: Training = 0.75 :: Test = 0.54\n",
      "Epoch 3\n",
      "Loss = 2.1243440661361386 :: Training = 0.53 :: Test = 0.43\n",
      "Epoch 4\n",
      "Loss = 1.9804486064833529 :: Training = 0.57 :: Test = 0.45\n",
      "Epoch 5\n",
      "Loss = 1.7400926014109637 :: Training = 0.74 :: Test = 0.6\n",
      "Epoch 6\n",
      "Loss = 1.185099790152633 :: Training = 0.88 :: Test = 0.67\n",
      "Epoch 7\n",
      "Loss = 1.0147737936988228 :: Training = 0.86 :: Test = 0.62\n",
      "Epoch 8\n",
      "Loss = 0.5700326590710398 :: Training = 0.93 :: Test = 0.69\n",
      "Epoch 9\n",
      "Loss = 0.6820014922001253 :: Training = 0.95 :: Test = 0.67\n",
      "Epoch 10\n",
      "Loss = 0.3162751866790271 :: Training = 1.0 :: Test = 0.68\n"
     ]
    }
   ],
   "source": [
    "# Design a two hidden layer architecture with dense layer, dropout layer\n",
    "# and ReLU as activation function\n",
    "def fcn_dropout_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 500)\n",
    "    relu_01 = ReLU()\n",
    "    dropout = Dropout()\n",
    "    hidden_02 = FullyConnected(500, 100)\n",
    "    relu_02 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, dropout, hidden_02, relu_02, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_dropout = NeuralNetwork(fcn_dropout_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn_dropout = Optimizer.sgd(fcn_dropout, train_images_batch, train_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=test_images_batch, y_test=test_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded\n"
     ]
    }
   ],
   "source": [
    "Cifar.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/neural-networks/Assignment_03/htw_nn_framework/cifar-10-batches-py/data_batch_1\n",
      "/storage/neural-networks/Assignment_03/htw_nn_framework/cifar-10-batches-py/data_batch_2\n",
      "/storage/neural-networks/Assignment_03/htw_nn_framework/cifar-10-batches-py/data_batch_3\n",
      "/storage/neural-networks/Assignment_03/htw_nn_framework/cifar-10-batches-py/data_batch_4\n",
      "/storage/neural-networks/Assignment_03/htw_nn_framework/cifar-10-batches-py/data_batch_5\n"
     ]
    }
   ],
   "source": [
    "tr_images, tr_labels, te_images, te_labels = Cifar.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(len(tr_images))\n",
    "tr_images, tr_labels = tr_images[shuffle_index], tr_labels[shuffle_index]\n",
    "\n",
    "tr_images_batch, tr_labels_batch, te_images_batch, te_labels_batch = tr_images[:100,:,:,:], tr_labels[:100], te_images[:100,:,:,:], te_labels[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 3.2050775279514725 :: Training = 0.11 :: Test = 0.1\n",
      "Epoch 2\n",
      "Loss = 3.011559622419788 :: Training = 0.14 :: Test = 0.15\n",
      "Epoch 3\n",
      "Loss = 2.3160568939061332 :: Training = 0.21 :: Test = 0.11\n",
      "Epoch 4\n",
      "Loss = 2.2307650724017205 :: Training = 0.37 :: Test = 0.26\n",
      "Epoch 5\n",
      "Loss = 2.125600435479874 :: Training = 0.37 :: Test = 0.26\n",
      "Epoch 6\n",
      "Loss = 2.078425141977356 :: Training = 0.49 :: Test = 0.31\n",
      "Epoch 7\n",
      "Loss = 1.9929621590876165 :: Training = 0.4 :: Test = 0.18\n",
      "Epoch 8\n",
      "Loss = 2.083024423177691 :: Training = 0.41 :: Test = 0.26\n",
      "Epoch 9\n",
      "Loss = 2.0006847247076744 :: Training = 0.3 :: Test = 0.21\n",
      "Epoch 10\n",
      "Loss = 2.2647431238408116 :: Training = 0.2 :: Test = 0.12\n",
      "Epoch 11\n",
      "Loss = 2.0206628191039777 :: Training = 0.38 :: Test = 0.27\n",
      "Epoch 12\n",
      "Loss = 1.810956127629157 :: Training = 0.43 :: Test = 0.26\n",
      "Epoch 13\n",
      "Loss = 1.8284005897026436 :: Training = 0.46 :: Test = 0.21\n",
      "Epoch 14\n",
      "Loss = 1.6347566910556055 :: Training = 0.32 :: Test = 0.2\n",
      "Epoch 15\n",
      "Loss = 2.158371192572245 :: Training = 0.26 :: Test = 0.16\n",
      "Epoch 16\n",
      "Loss = 2.1367916671267646 :: Training = 0.32 :: Test = 0.18\n",
      "Epoch 17\n",
      "Loss = 1.853823986646668 :: Training = 0.5 :: Test = 0.23\n",
      "Epoch 18\n",
      "Loss = 1.8506081594526718 :: Training = 0.55 :: Test = 0.25\n",
      "Epoch 19\n",
      "Loss = 1.8741843188346914 :: Training = 0.59 :: Test = 0.27\n",
      "Epoch 20\n",
      "Loss = 1.8617404209402488 :: Training = 0.41 :: Test = 0.23\n",
      "Epoch 21\n",
      "Loss = 1.75945400999877 :: Training = 0.62 :: Test = 0.29\n",
      "Epoch 22\n",
      "Loss = 1.458041856639175 :: Training = 0.57 :: Test = 0.19\n",
      "Epoch 23\n",
      "Loss = 1.532044571927463 :: Training = 0.64 :: Test = 0.23\n",
      "Epoch 24\n",
      "Loss = 1.5580258661260837 :: Training = 0.53 :: Test = 0.18\n",
      "Epoch 25\n",
      "Loss = 2.7743732231679514 :: Training = 0.27 :: Test = 0.17\n",
      "Epoch 26\n",
      "Loss = 2.2389844932525427 :: Training = 0.34 :: Test = 0.18\n",
      "Epoch 27\n",
      "Loss = 1.7863960684325793 :: Training = 0.52 :: Test = 0.29\n",
      "Epoch 28\n",
      "Loss = 1.692077076730556 :: Training = 0.53 :: Test = 0.29\n",
      "Epoch 29\n",
      "Loss = 1.413598169952985 :: Training = 0.58 :: Test = 0.28\n",
      "Epoch 30\n",
      "Loss = 1.474074640866859 :: Training = 0.47 :: Test = 0.23\n",
      "Epoch 31\n",
      "Loss = 1.5504606550276026 :: Training = 0.56 :: Test = 0.27\n",
      "Epoch 32\n",
      "Loss = 1.483138330874191 :: Training = 0.49 :: Test = 0.23\n",
      "Epoch 33\n",
      "Loss = 1.6861124793827855 :: Training = 0.58 :: Test = 0.28\n",
      "Epoch 34\n",
      "Loss = 1.5564300906455555 :: Training = 0.52 :: Test = 0.29\n",
      "Epoch 35\n",
      "Loss = 1.7670860918431173 :: Training = 0.57 :: Test = 0.2\n",
      "Epoch 36\n",
      "Loss = 1.6255328339437656 :: Training = 0.55 :: Test = 0.26\n",
      "Epoch 37\n",
      "Loss = 1.4859686627872788 :: Training = 0.7 :: Test = 0.28\n",
      "Epoch 38\n",
      "Loss = 1.1495152880772426 :: Training = 0.68 :: Test = 0.27\n",
      "Epoch 39\n",
      "Loss = 1.9332537427435605 :: Training = 0.38 :: Test = 0.2\n",
      "Epoch 40\n",
      "Loss = 2.559593098174776 :: Training = 0.49 :: Test = 0.19\n",
      "Epoch 41\n",
      "Loss = 1.4151799584038902 :: Training = 0.61 :: Test = 0.26\n",
      "Epoch 42\n",
      "Loss = 1.0474088097942131 :: Training = 0.72 :: Test = 0.29\n",
      "Epoch 43\n",
      "Loss = 1.197935072470417 :: Training = 0.6 :: Test = 0.31\n",
      "Epoch 44\n",
      "Loss = 0.8929133941717929 :: Training = 0.74 :: Test = 0.31\n",
      "Epoch 45\n",
      "Loss = 0.8648800803017141 :: Training = 0.73 :: Test = 0.25\n",
      "Epoch 46\n",
      "Loss = 1.4880274223305345 :: Training = 0.62 :: Test = 0.18\n",
      "Epoch 47\n",
      "Loss = 0.8534551660265279 :: Training = 0.79 :: Test = 0.31\n",
      "Epoch 48\n",
      "Loss = 0.9872223428893315 :: Training = 0.55 :: Test = 0.21\n",
      "Epoch 49\n",
      "Loss = 2.452755822005911 :: Training = 0.45 :: Test = 0.25\n",
      "Epoch 50\n",
      "Loss = 2.527990781127068 :: Training = 0.67 :: Test = 0.23\n"
     ]
    }
   ],
   "source": [
    "# Design a three hidden layer architecture with dense layer\n",
    "# and ReLU as activation function\n",
    "def fcn_cifar():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(3072, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_c10 = NeuralNetwork(fcn_cifar(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn_c10 = Optimizer.sgd(fcn_c10, tr_images_batch, tr_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=50, learning_rate=0.001, X_test=te_images_batch, y_test=te_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.302611298239187 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 2\n",
      "Loss = 2.296164235318004 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 3\n",
      "Loss = 2.2912339860886752 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 4\n",
      "Loss = 2.2875029678894325 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 5\n",
      "Loss = 2.284720066592887 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 6\n",
      "Loss = 2.2826847456867956 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 7\n",
      "Loss = 2.2812155066488415 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 8\n",
      "Loss = 2.2801752293490347 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 9\n",
      "Loss = 2.2794518576889704 :: Training = 0.17 :: Test = 0.16\n",
      "Epoch 10\n",
      "Loss = 2.279001287718948 :: Training = 0.17 :: Test = 0.16\n"
     ]
    }
   ],
   "source": [
    "# Design a one hidden layer architecture with conv layer\n",
    "# and ReLU as activation function\n",
    "def cnn_cifar():\n",
    "    conv = Conv(tr_images_batch.shape, 1, 3, 1, True)\n",
    "    relu_01 = ReLU()\n",
    "    flat = Flatten()\n",
    "    ouput = FullyConnected(1024, 10)\n",
    "    return [conv, relu_01, flat, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "cnn_c10 = NeuralNetwork(cnn_cifar(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "cnn_c10 = Optimizer.sgd(cnn_c10, tr_images_batch, tr_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=te_images_batch, y_test=te_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.3345305906717284 :: Training = 0.16 :: Test = 0.12\n",
      "Epoch 2\n",
      "Loss = 2.32878022929078 :: Training = 0.19 :: Test = 0.12\n",
      "Epoch 3\n",
      "Loss = 2.324507580853724 :: Training = 0.2 :: Test = 0.13\n",
      "Epoch 4\n",
      "Loss = 2.3169866420230623 :: Training = 0.2 :: Test = 0.16\n",
      "Epoch 5\n",
      "Loss = 2.305752171928516 :: Training = 0.2 :: Test = 0.16\n",
      "Epoch 6\n",
      "Loss = 2.289041711114825 :: Training = 0.21 :: Test = 0.15\n",
      "Epoch 7\n",
      "Loss = 2.2732884881915734 :: Training = 0.25 :: Test = 0.15\n",
      "Epoch 8\n",
      "Loss = 2.244754289155251 :: Training = 0.3 :: Test = 0.15\n",
      "Epoch 9\n",
      "Loss = 2.2226558763260735 :: Training = 0.31 :: Test = 0.16\n",
      "Epoch 10\n",
      "Loss = 2.202152812345323 :: Training = 0.29 :: Test = 0.16\n",
      "Epoch 11\n",
      "Loss = 2.183739159350683 :: Training = 0.3 :: Test = 0.15\n",
      "Epoch 12\n",
      "Loss = 2.167967877684971 :: Training = 0.31 :: Test = 0.15\n",
      "Epoch 13\n",
      "Loss = 2.153339349026197 :: Training = 0.33 :: Test = 0.13\n",
      "Epoch 14\n",
      "Loss = 2.1401278292213695 :: Training = 0.35 :: Test = 0.14\n",
      "Epoch 15\n",
      "Loss = 2.1280550011406603 :: Training = 0.36 :: Test = 0.14\n",
      "Epoch 16\n",
      "Loss = 2.1163841573684947 :: Training = 0.37 :: Test = 0.14\n",
      "Epoch 17\n",
      "Loss = 2.1047997010808457 :: Training = 0.38 :: Test = 0.14\n",
      "Epoch 18\n",
      "Loss = 2.0936579453053143 :: Training = 0.39 :: Test = 0.15\n",
      "Epoch 19\n",
      "Loss = 2.082509075939976 :: Training = 0.41 :: Test = 0.15\n",
      "Epoch 20\n",
      "Loss = 2.070796987689953 :: Training = 0.43 :: Test = 0.15\n",
      "Epoch 21\n",
      "Loss = 2.059061712589253 :: Training = 0.44 :: Test = 0.15\n",
      "Epoch 22\n",
      "Loss = 2.048785291712788 :: Training = 0.46 :: Test = 0.15\n",
      "Epoch 23\n",
      "Loss = 2.037268769339736 :: Training = 0.46 :: Test = 0.15\n",
      "Epoch 24\n",
      "Loss = 2.025454678317699 :: Training = 0.47 :: Test = 0.15\n",
      "Epoch 25\n",
      "Loss = 2.0135695823407653 :: Training = 0.48 :: Test = 0.14\n",
      "Epoch 26\n",
      "Loss = 2.0026625009146133 :: Training = 0.49 :: Test = 0.13\n",
      "Epoch 27\n",
      "Loss = 1.991374673932544 :: Training = 0.5 :: Test = 0.13\n",
      "Epoch 28\n",
      "Loss = 1.979314802843761 :: Training = 0.51 :: Test = 0.13\n",
      "Epoch 29\n",
      "Loss = 1.967898284223172 :: Training = 0.53 :: Test = 0.14\n",
      "Epoch 30\n",
      "Loss = 1.9555194945667624 :: Training = 0.55 :: Test = 0.14\n"
     ]
    }
   ],
   "source": [
    "def cnn_pool_cifar():\n",
    "    conv = Conv(tr_images_batch.shape, 1, 3, 1, True)\n",
    "    relu_01 = ReLU()\n",
    "    pool = Pool(tr_images_batch.shape, np.max, 2, 1)\n",
    "    flat = Flatten()\n",
    "    ouput = FullyConnected(961, 10)\n",
    "    return [conv, relu_01, pool, flat, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "cnn_pool_c10 = NeuralNetwork(cnn_pool_cifar(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "cnn_pool_c10 = Optimizer.sgd(cnn_pool_c10, tr_images_batch, tr_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=30, learning_rate=0.001, X_test=te_images_batch, y_test=te_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Conected Neural Network with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 3.171042149551644 :: Training = 0.11 :: Test = 0.15\n",
      "Epoch 2\n",
      "Loss = 3.3207889598836373 :: Training = 0.14 :: Test = 0.1\n",
      "Epoch 3\n",
      "Loss = 3.124752057661935 :: Training = 0.14 :: Test = 0.15\n",
      "Epoch 4\n",
      "Loss = 2.4397542517592354 :: Training = 0.18 :: Test = 0.16\n",
      "Epoch 5\n",
      "Loss = 2.321990831515732 :: Training = 0.31 :: Test = 0.19\n",
      "Epoch 6\n",
      "Loss = 2.3172580996097265 :: Training = 0.33 :: Test = 0.18\n",
      "Epoch 7\n",
      "Loss = 2.2882203194427713 :: Training = 0.26 :: Test = 0.23\n",
      "Epoch 8\n",
      "Loss = 2.1499055493228174 :: Training = 0.33 :: Test = 0.2\n",
      "Epoch 9\n",
      "Loss = 2.3541742678051456 :: Training = 0.32 :: Test = 0.23\n",
      "Epoch 10\n",
      "Loss = 2.2537620045545275 :: Training = 0.41 :: Test = 0.2\n",
      "Epoch 11\n",
      "Loss = 2.1118546376174376 :: Training = 0.4 :: Test = 0.3\n",
      "Epoch 12\n",
      "Loss = 2.169984427505885 :: Training = 0.44 :: Test = 0.24\n",
      "Epoch 13\n",
      "Loss = 2.20394506661958 :: Training = 0.44 :: Test = 0.26\n",
      "Epoch 14\n",
      "Loss = 2.295369757625976 :: Training = 0.33 :: Test = 0.16\n",
      "Epoch 15\n",
      "Loss = 2.174530732548236 :: Training = 0.51 :: Test = 0.27\n",
      "Epoch 16\n",
      "Loss = 2.0857489503132762 :: Training = 0.28 :: Test = 0.18\n",
      "Epoch 17\n",
      "Loss = 2.314700033424885 :: Training = 0.37 :: Test = 0.12\n",
      "Epoch 18\n",
      "Loss = 2.1874496238266987 :: Training = 0.29 :: Test = 0.11\n",
      "Epoch 19\n",
      "Loss = 2.2029340171710605 :: Training = 0.41 :: Test = 0.27\n",
      "Epoch 20\n",
      "Loss = 2.011892254864116 :: Training = 0.4 :: Test = 0.29\n",
      "Epoch 21\n",
      "Loss = 2.1202080392558873 :: Training = 0.38 :: Test = 0.27\n",
      "Epoch 22\n",
      "Loss = 1.8176046344864103 :: Training = 0.46 :: Test = 0.26\n",
      "Epoch 23\n",
      "Loss = 2.157747746665208 :: Training = 0.33 :: Test = 0.19\n",
      "Epoch 24\n",
      "Loss = 1.8363532935690874 :: Training = 0.49 :: Test = 0.28\n",
      "Epoch 25\n",
      "Loss = 1.9622482594052437 :: Training = 0.55 :: Test = 0.2\n",
      "Epoch 26\n",
      "Loss = 1.978055424110272 :: Training = 0.53 :: Test = 0.24\n",
      "Epoch 27\n",
      "Loss = 1.958986044796822 :: Training = 0.45 :: Test = 0.2\n",
      "Epoch 28\n",
      "Loss = 1.646090350314377 :: Training = 0.57 :: Test = 0.29\n",
      "Epoch 29\n",
      "Loss = 2.1949522281849774 :: Training = 0.32 :: Test = 0.17\n",
      "Epoch 30\n",
      "Loss = 1.9193662633384294 :: Training = 0.61 :: Test = 0.31\n",
      "Epoch 31\n",
      "Loss = 1.8565574828896294 :: Training = 0.46 :: Test = 0.2\n",
      "Epoch 32\n",
      "Loss = 2.0253188647803144 :: Training = 0.56 :: Test = 0.22\n",
      "Epoch 33\n",
      "Loss = 1.8082263253992639 :: Training = 0.54 :: Test = 0.26\n",
      "Epoch 34\n",
      "Loss = 1.9063896316726696 :: Training = 0.5 :: Test = 0.25\n",
      "Epoch 35\n",
      "Loss = 2.1743309135196287 :: Training = 0.27 :: Test = 0.18\n",
      "Epoch 36\n",
      "Loss = 1.9542533105441682 :: Training = 0.39 :: Test = 0.17\n",
      "Epoch 37\n",
      "Loss = 1.677849953747911 :: Training = 0.59 :: Test = 0.22\n",
      "Epoch 38\n",
      "Loss = 1.6755024667220955 :: Training = 0.67 :: Test = 0.25\n",
      "Epoch 39\n",
      "Loss = 1.874085265333754 :: Training = 0.59 :: Test = 0.27\n",
      "Epoch 40\n",
      "Loss = 1.7603602663276645 :: Training = 0.56 :: Test = 0.2\n"
     ]
    }
   ],
   "source": [
    "# Design a two hidden layer architecture with dense layer, dropout layer\n",
    "# and ReLU as activation function\n",
    "def fcn_dropout_cifar():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(3072, 500)\n",
    "    relu_01 = ReLU()\n",
    "    dropout = Dropout()\n",
    "    hidden_02 = FullyConnected(500, 100)\n",
    "    relu_02 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, dropout, hidden_02, relu_02, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_dropout_c10 = NeuralNetwork(fcn_dropout_cifar(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn_dropout_c10 = Optimizer.sgd(fcn_dropout_c10, tr_images_batch, tr_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=40, learning_rate=0.001, X_test=te_images_batch, y_test=te_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
