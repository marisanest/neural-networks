{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# used to check whether datasets need to be downloaded\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):  \n",
    "    with open(filename, 'rb') as f:      \n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        # Create a numpy array with dimensions (n, channel, height, width)\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float64\")\n",
    "        # Squash all values into interval [0,1]\n",
    "        X = X / 256.\n",
    "        Y = np.array(Y).astype(\"uint8\")\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(root_folder):\n",
    "    \n",
    "    Xtr = np.zeros((50000,3,32,32))\n",
    "    Ytr = np.zeros((50000)).astype(\"uint8\")\n",
    "    batch_size = 10000\n",
    "    \n",
    "    # Load and decode each batch of the trainings set\n",
    "    for index, batch in enumerate(range(1,6)):\n",
    "        f = os.path.join(root_folder, 'data_batch_%d' % batch)\n",
    "        print(f)\n",
    "        \n",
    "        # Boundary indices of the current batch within the overall trainings set\n",
    "        b_i_start = index * batch_size\n",
    "        b_i_end = b_i_start + batch_size\n",
    "        \n",
    "        # Load and decode trainigs data and labels\n",
    "        Xtr[b_i_start:b_i_end], Ytr[b_i_start:b_i_end] = load_CIFAR_batch(f)\n",
    "\n",
    "    # Load and decode test data and labels  \n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(root_folder, 'test_batch'))\n",
    "    \n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "cifar10_dir = 'cifar-10-batches-py'  # default dir\n",
    "\n",
    "if not Path('cifar-10-batches-py').is_dir():\n",
    "    print('Downloading data sets...')\n",
    "    !sh get_datasets.sh\n",
    "else:\n",
    "    print('Data sets already downloaded!')\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    ''' Flatten layer used to reshape inputs into vector representation\n",
    "    \n",
    "    Layer should be used in the forward pass before a dense layer to \n",
    "    transform a given tensor into a vector. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Reshapes a n-dim representation into a vector \n",
    "            by preserving the number of input rows.\n",
    "        \n",
    "        Examples:\n",
    "            [10000,[1,28,28]] -> [10000,784]\n",
    "        '''\n",
    "        self.X_shape = X.shape\n",
    "        self.out_shape = (self.X_shape[0], -1)    \n",
    "        out = X.reshape(-1).reshape(self.out_shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Restore dimensions before flattening operation\n",
    "        '''\n",
    "        out = dout.reshape(self.X_shape)\n",
    "        return out, []\n",
    "\n",
    "class FullyConnected():\n",
    "    ''' Fully connected layer implemtenting linear function hypothesis \n",
    "        in the forward pass and its derivation in the backward pass.\n",
    "    '''\n",
    "    def __init__(self, in_size, out_size):\n",
    "        ''' Initilize all learning parameters in the layer\n",
    "        \n",
    "        Weights will be initilized with modified Xavier initialization.\n",
    "        Biases will be initilized with zero. \n",
    "        '''\n",
    "        self.W = np.random.randn(in_size, out_size) # np.sqrt(2. / in_size)\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        out = np.add(np.dot(self.X, self.W), self.b)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        dW = np.dot(self.X.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dX, [dW, db]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    ''' Implements activation function rectified linear unit (ReLU) \n",
    "    \n",
    "    ReLU activation function is defined as the positive part of \n",
    "    its argument. Todo: insert arxiv paper reference\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' In the forward pass return the identity for x < 0\n",
    "        \n",
    "        Safe input for backprop and forward all values that are above 0.\n",
    "        '''\n",
    "        self.X = X\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Derivative of ReLU\n",
    "        \n",
    "        Retruns:\n",
    "            dX: for all x \\elem X <= 0 in forward pass \n",
    "                return 0 else x\n",
    "            []: no gradients on ReLU operation\n",
    "        '''\n",
    "        dX = dout.copy()\n",
    "        dX[self.X <= 0] = 0\n",
    "        return dX, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    ''' Creates a neural network from a given layer architecture \n",
    "    \n",
    "    This class is suited for fully connected network and\n",
    "    convolutional neural network architectures. It connects \n",
    "    the layers and passes the data from one end to another.\n",
    "    '''\n",
    "    def __init__(self, layers, score_func=None):\n",
    "        ''' Setup a global parameter list and initilize a\n",
    "            score function that is used for predictions.\n",
    "        \n",
    "        Args:\n",
    "            layer: neural network architecture based on layer and activation function objects\n",
    "            score_func: function that is used as classifier on the output\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params.append(layer.params)\n",
    "        self.score_func = score_func\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Pass input X through all layers in the network \n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = []\n",
    "        ''' Backprop through the network and keep a list of the gradients\n",
    "            from each layer.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Run a forward pass and use the score function to classify \n",
    "            the output.\n",
    "        '''\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(self.score_func(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossCriteria():\n",
    "    ''' Implements diffrent typs of loss and score functions for neural networks\n",
    "    \n",
    "    Todo:\n",
    "        - Implement init that defines score and loss function \n",
    "    '''\n",
    "    def softmax(X):\n",
    "        ''' Numeric stable calculation of softmax\n",
    "        '''\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_softmax(X, y):\n",
    "        ''' Computes loss and prepares dout for backprop \n",
    "\n",
    "        https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        '''\n",
    "        m = y.shape[0]\n",
    "        p = LossCriteria.softmax(X)\n",
    "        log_likelihood = -np.log(p[range(m), y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        dout = p.copy()\n",
    "        dout[range(m), y] -= 1\n",
    "        return loss, dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer():   \n",
    "    def get_minibatches(X, y, batch_size):\n",
    "        ''' Decomposes data set into small subsets (batch)\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i + batch_size, :, :, :]\n",
    "            y_batch = y[i:i + batch_size, ]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches    \n",
    "\n",
    "    def sgd(network, X_train, y_train, loss_function, batch_size=32, epoch=100, learning_rate=0.001, X_test=None, y_test=None, verbose=None):\n",
    "        ''' Optimize a given network with stochastic gradient descent \n",
    "        '''\n",
    "        minibatches = Optimizer.get_minibatches(X_train, y_train, batch_size)\n",
    "        for i in range(epoch):\n",
    "            loss = 0\n",
    "            if verbose:\n",
    "                print('Epoch',i + 1)\n",
    "            for X_mini, y_mini in minibatches:\n",
    "                # calculate loss and derivation of the last layer\n",
    "                loss, dout = loss_function(network.forward(X_mini), y_mini)\n",
    "                # calculate gradients via backpropagation\n",
    "                grads = network.backward(dout)\n",
    "                # run vanilla sgd update for all learnable parameters in self.params\n",
    "                for param, grad in zip(network.params, reversed(grads)):\n",
    "                    for i in range(len(grad)):\n",
    "                        param[i] += - learning_rate * grad[i]\n",
    "            if verbose:\n",
    "                train_acc = np.mean(y_train == network.predict(X_train))\n",
    "                test_acc = np.mean(y_test == network.predict(X_test))                                \n",
    "                print(\"Loss = {0} :: Training = {1} :: Test = {2}\".format(loss, train_acc, test_acc))\n",
    "        return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images_batch, train_labels_batch, test_images_batch, test_labels_batch = train_images[:100,:,:,:], train_labels[:100], test_images[:100,:,:,:], test_labels[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(3072, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images_batch, train_labels_batch, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.01, X_test=test_images_batch, y_test=test_labels_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
